---
title: 'Statistics Background for Metabolomics'
subtitle: 'Statistics tests and related issuess'
author: Alex Sanchez-Pla
institute: "Genetics, Microbiology & Statistics Department <br> Universitat de Barcelona"
date: "`r Sys.Date()`"
output:
  xaringan::moon_reader:
    css: [default, metropolis, metropolis-fonts, "mycss.css"]
    lib_dir: libs
    nature:
      ratio: '16:9'
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: true
editor_options: 
  chunk_output_type: console
---

<style type="text/css">
.remark-slide-content {
    font-size: 22px;
    padding: 1em 4em 1em 4em;
}
.left-code {
  color: #777;
  width: 38%;
  height: 92%;
  float: left;
}
.right-plot {
  width: 60%;
  float: right;
  padding-left: 1%;
}
</style>

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE, echo=FALSE,
        message=FALSE,warning=FALSE,
        fig.dim=c(4.8, 4.5), fig.retina=2, out.width="100%")

knitr::opts_chunk$set(echo = FALSE)

knitr::knit_hooks$set(mysize = function(before, options, envir) {
  if (before) 
    return(options$size)
})
```


# Outline

.coliumnwide[
### The class comparison problem

### Statistical tests
#### Calculation of p-values
#### Permutations tests
#### The volcano plot
### Multiple testing

### Examples
]

---

class: inverse, middle, center

name: StatisticalTests
  
# Statistical tests

---



---
                                    
#     The *Class comparison* problem 

- Main goal: Identifying  significantly different features

- Identify features whose values are (significantly)
    associated with different conditions
     - Treatment, cell type,... (qualitative covariates)
     - Dose, time, ... (quantitative covariate)
     - Survival, infection time,... !
     
-   Estimate effects/differences between groups,

  - either directly: $D = Y - X$ or
  - in a log scale (using ratios): $log(X)-log(Y) [=log(X/Y)]$.

---

#        What is a “significant change”?

.pull-left[

- Depends on the variability
    within groups.
    
- Variability, of course,  may be
    different from feature to
    feature.
    
-   To assess the statistical
    significance of observed differences,
    a statistical test is usually condected *for
    each feature*.
    
- There also exist multivariate tests to make all comparisons at one, but the nature of the data usually makes them unfeasible.
]

.pull-right[

![Plot title. ](1-StatisticsBackground-2_insertimage_1.png)
]




---

#     Different settings for statistical tests (1)

- **Indirect comparisons**: 2 groups, 2 samples, unpaired
    - E.g. **10** individuals: 5 suffer diabetes, 5 healthy
    - One sample fro each individual
    - Typically: Two sample t-test or similar

```{r out.width="100%", fig.cap=''}
knitr::include_graphics("1-StatisticsBackground-2_insertimage_3.png")
```  

---

# Different settings for statistical tests (2)
    
-  ** Direct comparisons:** Two groups, two samples, **paired**
    - E.g. 6 individuals with brain stroke.
    - Two samples from each: one from healthy (region 1) and one
        from affected (region 2). That is a total of 2*6 = 12 samples
    -   Typically: One sample t-test (also called paired t-test) or
        similar, based on the individual differences between
        conditions.

```{r out.width="80%", fig.cap=''}
knitr::include_graphics("1-StatisticsBackground-2_insertimage_4.png")
```        

---

#   Some issues in feature selection

- Each technology’s data may have peculiarities that
  have to be dealt with.
- Some related with small sample sizes
   -   Variance unstability
   -   Non-normality of the data
- Other related to big number of variables
   -   Multiple testing


---

# Variance unstability

- Can we trust average effect sizes (average
      difference of means) alone?
-     Can we trust the t-statistic alone?
-     Here is evidence that the answer is no.


```{r out.width="80%", fig.cap=''}
knitr::include_graphics("1-StatisticsBackground-2_insertimage_5.png")
```  


---

# Variance unstability (1): outliers
 - Can we trust average effect sizes (average
      difference of means) alone?
 -    Can we trust the t statistic alone?
 -    Here is evidence that the answer is no.
 

```{r out.width="80%", fig.cap='Averages can be driven by outliers'}
knitr::include_graphics("1-StatisticsBackground-2_insertimage_6.png")
```


---

# Variance unstability (2): tiny variances
- Can we trust average effect sizes (average
       difference of means) alone?
-      Can we trust the t statistic alone?
-      Here is evidence that the answer is no.

```{r out.width="80%", fig.cap='t-values can be driven by tiny variances'}

knitr::include_graphics("1-StatisticsBackground-2_insertimage_7.png")
```

---

#  Solutions: Adapt t-tests

.pull-left[

- A standard solution: Combine

  - Local estimates of variability, $SE_g$ (based on individual features)
  
  - With global estimates, $SE$, (based on all features together)
  
- This results in *moderated estimators* that account simultaneosuly for
  - The variability of individual features
  - And that of all features together.
  
]

.pull-right[

```{r out.width="100%", fig.cap=''}
knitr::include_graphics("1-StatisticsBackground-2_insertimage_8.png")
```
]
---

#     Up to here…: 

- Can we generate a list of candidate features?
- With the tools we have, the reasonable steps to generate a
list of candidate features may be:

```{r out.width="80%", fig.cap=''}
knitr::include_graphics("1-StatisticsBackground-2_insertimage_9.png")
```
- We need to be able to figure how significant are these values.
- Traditional, somehow polemic, approach: 
  - Assign them p-values
  - Use these to select those features to be retained (* But see later*)

                                                                        ---
                                                                        # Nominal p-values
                                                                                 
- After a test statistic is computed, it is convenient to convert it to
   a p-value:

- It is defined as *The probability that a test statistic, say $S(X)$, takes values equal or greater than the observed value, say $X_0$, under the assumption that the null hypothesis is true
$$
                p=P\{S(X)>=S(X_0)|H_0 \mbox{ true}\}
$$

---

# Significance testing

- Test of significance at the $\alpha$ level:

  - Reject the null hypothesis if your p-value is smaller
    than the significance level
  - It has advantages but not free from criticisms
  
- Features with p-values falling below a prescribed
  level may be regarded as significant
  
- As we know, depending on what the truth is this can lead to

  - Two type of correct decisions
  
  - Two possible types of errors

---

#  Hypothesis testing overview

```{r out.width="100%", fig.cap=''}
knitr::include_graphics("1-StatisticsBackground-2_insertimage_10.png")
```

---

#   Calculation of p-values (1)

- Standard methods for calculating p-values is to use tabulated p-values *for the distribution that the test statistic is assumed to follow*.

- This, however, maybe harder to check than one would expect.

  - In the toy dataset, where each variable has onle 6 observations the normality assumption is impossible to check!
  
  - If sample size is bigger it may be possible to do some goodness of fit test, but, it should be done carefully and for all features in the dataset.
  
- This kind of checks are usually omitted and the *validity conditions are assumed to be true*

  - For some distributions, where there is robustness to departure of assumptions, it may work in a wide range of conditions.
  
  - It may be a good idea to look for alternatives.
  
  
---

# Calculation of p-values (2) Permutations tests

- Permutation tests are a good alternative to parametric, or even non-parametric tests.

- Based on data shuffling. No assumptions (only *exchangeability* is required.)

- Relatively simple to understand and implement. They are based on
  - Random interchange of labels between samples
  - Estimate p-values is based on the approximate permutation distribution of the test statistic.
  
---

# Permutation tests algorithm
  
- Repeat, for each feature $\mathbf{xi},\, i=1,...N$:
  - For every possible permutation $1,...B$ of its observations
    - Permute the $n$ data points for that feature. 
    - Design first $n_1$ as "treatments", the second $n_2$ as "controls"
    - Calculate the corresponding two sample test statistic, $t_b$
- After all the B permutations are done approximete the $p$-value by:

$$p =\frac{\# bº,:\, |t_b| ≥ |t_{observed}|}{B}$$

- Notice that **all these steps have to be performed for all features**, 
- that is, permutation tests are *computationally intensive*!

---

# Permutation tests (2)

```{r out.width="100%", fig.cap=''}
knitr::include_graphics("1-StatisticsBackground-2_insertimage_11.png")
```

---

#  The volcano plot: fold change vs -log(pvalue)

```{r out.width="100%", fig.cap=''}
knitr::include_graphics("1-StatisticsBackground-2_insertimage_12.png")
```


---

class: inverse, middle, center

name: multipletesting

# Multipletesting
  
---

#    The Multiple Testing problem

- Whatever approach we use to detect significant differences in features there is a common characteristic: 

- *Every test is applied to every feature set in a long collection of features*

- This leads to a *multiple testing problem*: 

  - As the number of tests increases
  
  - The probability of observing at least one false positive is also going to increase
  
- In order to avoid an artificial inflation of *False positive discoveries* some adjustment (also called "corrections") are recommended.


---

# Why multiple testing matters in omics

- The probability of observing one false positive if testing once is:

  - P(Making a type I error) = $\alpha$
  - P(not making a type I error) =  $1-\alpha$

- Now imagine we perform m tests independently
  - P(not making a type I error in $m$ tests) =  $(1-\alpha)^m$
  - P(making at least a type I error in $m$ tests) = $1-(1-\alpha)^m$
  
As $m$ increases the probability of having at least one type error tends to increase

---

# Type I error is not useful in omics

![](1-StatisticsBackground-2_insertimage_13.png)
---

# How can we deal with this issue?

- Controlling for type I error is not feasible if many tests.

- There are distinct strategies to deal with it:

  1. *Extend the idea of type I error*: FWER and FDR are two extensions that , somehow, modify the error rate with the aim aof providing a "global" control of error probability.
    
  2. *Look for procedures that control the probability of error for these extended error types*: Mainly, this means adjusting raw p-values.


- AN ANALOGY: Indiana Jone's bridge

  - *Would you cross a bridge once if the probability that it broke down is 0.001?*
  - *Would you cross it 10.000 times?*
  - *What would you do if you decided not to cross that bridge?* 

---

# Error rate extensions and p-value adjustments

- Family Wise Error Rate (FWER)

  - FWER is the probability of observing, at least, one false positive

- False Discovery Rate (FDR)

  - False Discovery Rate is the *expected value of proportion of false positives* among rejected null hypotheses.
  
- Each type of error rate can be associated with distinct types of p-value adjustments

  - Bonferroni method  is used to provide control of  FWER
  
  - Benjamini-Hochberg (q-value) is used to provide control of  FDR.

---

# Difference between FWER and FDR

- FWER Controls for no (0) false positives
  - Controlling FWER yields fewer features (false positives), 
  - but you are likely to miss many.
- FWR is adequate if goal is to identify few features that differ between two groups.

- FDR Controls the proportion of false positives
  - If you can tolerate more false positives 
  - you will get many fewer false negatives
- Adequate if goal is to pursue the study e.g. to determine functional relationships among features.

---

# Steps to generate a list of candidate features  (2)

```{r out.width="90%", fig.cap=''}
knitr::include_graphics("1-StatisticsBackground-2_insertimage_14.png")
```

---
# An example

- A list of 63 potentially significant p-values has been adjuested using Bonferroni and BH

- BH is clearly more restrictive than BH, which, however is more restrictive than the raw p-values.


```{r echo=FALSE}
cachexia.t_test <- read.csv("datasets/cachexia-t_test_all.csv", row.names=1)
rawPs <- cachexia.t_test$p.value
names(rawPs) <- row.names(cachexia.t_test)
bonfP <- p.adjust(rawPs, method= c("bonferroni"))
bhP <-  p.adjust(rawPs, method= c("BH"))
pVals <-data.frame(raw = rawPs, Bonferroni=bonfP, FDR = bhP)
Ordered <-round(pVals[order(pVals$raw),] ,6)
```

<small>
.pull-left[
```{r echo=FALSE}
kableExtra::kable(Ordered[1:8,])

```

P-values at the top of the table
]

.pull-right[
```{r echo=FALSE}
  kableExtra::kable(Ordered[41:48,])
```
P-values at the bottom of the table
]

</small>
---


# Questions?

class: inverse, middle, center

name: Resources
  
# References and Resources

---

