<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Statistics Background for Metabolomics</title>
    <meta charset="utf-8" />
    <meta name="author" content="Alex Sanchez-Pla" />
    <meta name="date" content="2024-06-11" />
    <script src="libs/header-attrs/header-attrs.js"></script>
    <link href="libs/remark-css/default.css" rel="stylesheet" />
    <link href="libs/remark-css/metropolis.css" rel="stylesheet" />
    <link href="libs/remark-css/metropolis-fonts.css" rel="stylesheet" />
    <link rel="stylesheet" href="mycss.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

.title[
# Statistics Background for Metabolomics
]
.subtitle[
## Statistics tests and related issuess
]
.author[
### Alex Sanchez-Pla
]
.institute[
### Genetics, Microbiology &amp; Statistics Department <br> Universitat de Barcelona
]
.date[
### 2024-06-11
]

---


&lt;style type="text/css"&gt;
.remark-slide-content {
    font-size: 22px;
    padding: 1em 4em 1em 4em;
}
.left-code {
  color: #777;
  width: 38%;
  height: 92%;
  float: left;
}
.right-plot {
  width: 60%;
  float: right;
  padding-left: 1%;
}
&lt;/style&gt;




# Outline

.coliumnwide[
### The class comparison problem

### Statistical tests
#### Calculation of p-values
#### Permutations tests
#### The volcano plot
### Multiple testing

### Examples
]

---

class: inverse, middle, center

name: StatisticalTests
  
# Statistical tests

---



---
                                    
#     The *Class comparison* problem 

- Main goal: Identifying  significantly different features

- Identify features whose values are (significantly)
    associated with different conditions
     - Treatment, cell type,... (qualitative covariates)
     - Dose, time, ... (quantitative covariate)
     - Survival, infection time,... !
     
-   Estimate effects/differences between groups,

  - either directly: `\(D = Y - X\)` or
  - in a log scale (using ratios): `\(log(X)-log(Y) [=log(X/Y)]\)`.

---

#        What is a “significant change”?

.pull-left[

- Depends on the variability
    within groups.
    
- Variability, of course,  may be
    different from feature to
    feature.
    
-   To assess the statistical
    significance of observed differences,
    a statistical test is usually condected *for
    each feature*.
    
- There also exist multivariate tests to make all comparisons at one, but the nature of the data usually makes them unfeasible.
]

.pull-right[

![Plot title. ](1-StatisticsBackground-2_insertimage_1.png)
]




---

#     Different settings for statistical tests (1)

- **Indirect comparisons**: 2 groups, 2 samples, unpaired
    - E.g. **10** individuals: 5 suffer diabetes, 5 healthy
    - One sample fro each individual
    - Typically: Two sample t-test or similar

&lt;img src="1-StatisticsBackground-2_insertimage_3.png" width="100%" /&gt;

---

# Different settings for statistical tests (2)
    
-  ** Direct comparisons:** Two groups, two samples, **paired**
    - E.g. 6 individuals with brain stroke.
    - Two samples from each: one from healthy (region 1) and one
        from affected (region 2). That is a total of 2*6 = 12 samples
    -   Typically: One sample t-test (also called paired t-test) or
        similar, based on the individual differences between
        conditions.

&lt;img src="1-StatisticsBackground-2_insertimage_4.png" width="80%" /&gt;

---

#   Some issues in feature selection

- Each technology’s data may have peculiarities that
  have to be dealt with.
- Some related with small sample sizes
   -   Variance unstability
   -   Non-normality of the data
- Other related to big number of variables
   -   Multiple testing


---

# Variance unstability

- Can we trust average effect sizes (average
      difference of means) alone?
-     Can we trust the t-statistic alone?
-     Here is evidence that the answer is no.


&lt;img src="1-StatisticsBackground-2_insertimage_5.png" width="80%" /&gt;


---

# Variance unstability (1): outliers
 - Can we trust average effect sizes (average
      difference of means) alone?
 -    Can we trust the t statistic alone?
 -    Here is evidence that the answer is no.
 

&lt;div class="figure"&gt;
&lt;img src="1-StatisticsBackground-2_insertimage_6.png" alt="Averages can be driven by outliers" width="80%" /&gt;
&lt;p class="caption"&gt;Averages can be driven by outliers&lt;/p&gt;
&lt;/div&gt;


---

# Variance unstability (2): tiny variances
- Can we trust average effect sizes (average
       difference of means) alone?
-      Can we trust the t statistic alone?
-      Here is evidence that the answer is no.

&lt;div class="figure"&gt;
&lt;img src="1-StatisticsBackground-2_insertimage_7.png" alt="t-values can be driven by tiny variances" width="80%" /&gt;
&lt;p class="caption"&gt;t-values can be driven by tiny variances&lt;/p&gt;
&lt;/div&gt;

---

#  Solutions: Adapt t-tests

.pull-left[

- A standard solution: Combine

  - Local estimates of variability, `\(SE_g\)` (based on individual features)
  
  - With global estimates, `\(SE\)`, (based on all features together)
  
- This results in *moderated estimators* that account simultaneosuly for
  - The variability of individual features
  - And that of all features together.
  
]

.pull-right[

&lt;img src="1-StatisticsBackground-2_insertimage_8.png" width="100%" /&gt;
]
---

#     Up to here…: 

- Can we generate a list of candidate features?
- With the tools we have, the reasonable steps to generate a
list of candidate features may be:

&lt;img src="1-StatisticsBackground-2_insertimage_9.png" width="80%" /&gt;
- We need to be able to figure how significant are these values.
- Traditional, somehow polemic, approach: 
  - Assign them p-values
  - Use these to select those features to be retained (* But see later*)

                                                                        ---
                                                                        # Nominal p-values
                                                                                 
- After a test statistic is computed, it is convenient to convert it to
   a p-value:

- It is defined as *The probability that a test statistic, say `\(S(X)\)`, takes values equal or greater than the observed value, say `\(X_0\)`, under the assumption that the null hypothesis is true
$$
                p=P\{S(X)&gt;=S(X_0)|H_0 \mbox{ true}\}
$$

---

# Significance testing

- Test of significance at the `\(\alpha\)` level:

  - Reject the null hypothesis if your p-value is smaller
    than the significance level
  - It has advantages but not free from criticisms
  
- Features with p-values falling below a prescribed
  level may be regarded as significant
  
- As we know, depending on what the truth is this can lead to

  - Two type of correct decisions
  
  - Two possible types of errors

---

#  Hypothesis testing overview

&lt;img src="1-StatisticsBackground-2_insertimage_10.png" width="100%" /&gt;

---

#   Calculation of p-values (1)

- Standard methods for calculating p-values is to use tabulated p-values *for the distribution that the test statistic is assumed to follow*.

- This, however, maybe harder to check than one would expect.

  - In the toy dataset, where each variable has onle 6 observations the normality assumption is impossible to check!
  
  - If sample size is bigger it may be possible to do some goodness of fit test, but, it should be done carefully and for all features in the dataset.
  
- This kind of checks are usually omitted and the *validity conditions are assumed to be true*

  - For some distributions, where there is robustness to departure of assumptions, it may work in a wide range of conditions.
  
  - It may be a good idea to look for alternatives.
  
  
---

# Calculation of p-values (2) Permutations tests

- Permutation tests are a good alternative to parametric, or even non-parametric tests.

- Based on data shuffling. No assumptions (only *exchangeability* is required.)

- Relatively simple to understand and implement. They are based on
  - Random interchange of labels between samples
  - Estimate p-values is based on the approximate permutation distribution of the test statistic.
  
---

# Permutation tests algorithm
  
- Repeat, for each feature `\(\mathbf{xi},\, i=1,...N\)`:
  - For every possible permutation `\(1,...B\)` of its observations
    - Permute the `\(n\)` data points for that feature. 
    - Design first `\(n_1\)` as "treatments", the second `\(n_2\)` as "controls"
    - Calculate the corresponding two sample test statistic, `\(t_b\)`
- After all the B permutations are done approximete the `\(p\)`-value by:

`$$p =\frac{\# bº,:\, |t_b| ≥ |t_{observed}|}{B}$$`

- Notice that **all these steps have to be performed for all features**, 
- that is, permutation tests are *computationally intensive*!

---

# Permutation tests (2)

&lt;img src="1-StatisticsBackground-2_insertimage_11.png" width="100%" /&gt;

---

#  The volcano plot: fold change vs -log(pvalue)

&lt;img src="1-StatisticsBackground-2_insertimage_12.png" width="100%" /&gt;


---

class: inverse, middle, center

name: multipletesting

# Multipletesting
  
---

#    The Multiple Testing problem

- Whatever approach we use to detect significant differences in features there is a common characteristic: 

- *Every test is applied to every feature set in a long collection of features*

- This leads to a *multiple testing problem*: 

  - As the number of tests increases
  
  - The probability of observing at least one false positive is also going to increase
  
- In order to avoid an artificial inflation of *False positive discoveries* some adjustment (also called "corrections") are recommended.


---

# Why multiple testing matters in omics

- The probability of observing one false positive if testing once is:

  - P(Making a type I error) = `\(\alpha\)`
  - P(not making a type I error) =  `\(1-\alpha\)`

- Now imagine we perform m tests independently
  - P(not making a type I error in `\(m\)` tests) =  `\((1-\alpha)^m\)`
  - P(making at least a type I error in `\(m\)` tests) = `\(1-(1-\alpha)^m\)`
  
As `\(m\)` increases the probability of having at least one type error tends to increase

---

# Type I error is not useful in omics

![](1-StatisticsBackground-2_insertimage_13.png)
---

# How can we deal with this issue?

- Controlling for type I error is not feasible if many tests.

- There are distinct strategies to deal with it:

  1. *Extend the idea of type I error*: FWER and FDR are two extensions that , somehow, modify the error rate with the aim aof providing a "global" control of error probability.
    
  2. *Look for procedures that control the probability of error for these extended error types*: Mainly, this means adjusting raw p-values.


- AN ANALOGY: Indiana Jone's bridge

  - *Would you cross a bridge once if the probability that it broke down is 0.001?*
  - *Would you cross it 10.000 times?*
  - *What would you do if you decided not to cross that bridge?* 

---

# Error rate extensions and p-value adjustments

- Family Wise Error Rate (FWER)

  - FWER is the probability of observing, at least, one false positive

- False Discovery Rate (FDR)

  - False Discovery Rate is the *expected value of proportion of false positives* among rejected null hypotheses.
  
- Each type of error rate can be associated with distinct types of p-value adjustments

  - Bonferroni method  is used to provide control of  FWER
  
  - Benjamini-Hochberg (q-value) is used to provide control of  FDR.

---

# Difference between FWER and FDR

- FWER Controls for no (0) false positives
  - Controlling FWER yields fewer features (false positives), 
  - but you are likely to miss many.
- FWR is adequate if goal is to identify few features that differ between two groups.

- FDR Controls the proportion of false positives
  - If you can tolerate more false positives 
  - you will get many fewer false negatives
- Adequate if goal is to pursue the study e.g. to determine functional relationships among features.

---

# Steps to generate a list of candidate features  (2)

&lt;img src="1-StatisticsBackground-2_insertimage_14.png" width="90%" /&gt;

---
# An example

- A list of 63 potentially significant p-values has been adjuested using Bonferroni and BH

- BH is clearly more restrictive than BH, which, however is more restrictive than the raw p-values.




&lt;small&gt;
.pull-left[

|                     |     raw| Bonferroni|      FDR|
|:--------------------|-------:|----------:|--------:|
|Quinolinate          | 3.0e-06|   0.000218| 0.000218|
|Glucose              | 1.6e-05|   0.001036| 0.000276|
|3-Hydroxyisovalerate | 1.9e-05|   0.001187| 0.000276|
|Leucine              | 2.0e-05|   0.001232| 0.000276|
|Succinate            | 2.9e-05|   0.001802| 0.000276|
|Valine               | 3.1e-05|   0.001922| 0.000276|
|N,N-Dimethylglycine  | 3.4e-05|   0.002125| 0.000276|
|Adipate              | 3.5e-05|   0.002206| 0.000276|

P-values at the top of the table
]

.pull-right[

|                           |      raw| Bonferroni|      FDR|
|:--------------------------|--------:|----------:|--------:|
|Trigonelline               | 0.005797|   0.365230| 0.008816|
|Hippurate                  | 0.005877|   0.370276| 0.008816|
|Trimethylamine N-oxide     | 0.006344|   0.399666| 0.009295|
|O-Acetylcarnitine          | 0.007151|   0.450507| 0.010239|
|Ethanolamine               | 0.008639|   0.544251| 0.012094|
|Glycine                    | 0.014320|   0.902160| 0.019612|
|Taurine                    | 0.019209|   1.000000| 0.025748|
|1,6-Anhydro-beta-D-glucose | 0.026248|   1.000000| 0.034230|
P-values at the bottom of the table
]

&lt;/small&gt;
---


# Questions?

class: inverse, middle, center

name: Resources
  
# References and Resources

---

    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"ratio": "16:9",
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": true
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
// add `data-at-shortcutkeys` attribute to <body> to resolve conflicts with JAWS
// screen reader (see PR #262)
(function(d) {
  let res = {};
  d.querySelectorAll('.remark-help-content table tr').forEach(tr => {
    const t = tr.querySelector('td:nth-child(2)').innerText;
    tr.querySelectorAll('td:first-child .key').forEach(key => {
      const k = key.innerText;
      if (/^[a-z]$/.test(k)) res[k] = t;  // must be a single letter (key)
    });
  });
  d.body.setAttribute('data-at-shortcutkeys', JSON.stringify(res));
})(document);
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
