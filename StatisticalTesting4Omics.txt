#                          Outline
- The class comparison problem
- Statistical tests
   -   Calculation of p-values
   -   Permutations tests
   -   The volcano plot
- Multiple testing
- Examples

---
                                    
#     Class comparison: Identifying

    significantly different features
- Identify metabolites whose expression is significantly
    associated with different conditions
     - Treatment, cell type,... (qualitative covariates)
     - Dose, time, ... (quantitative covariate)
     - Survival, infection time,... !
-   Estimate effects/differences between groups
    probably using log-ratios, i.e. the difference on log
    scale:
        log(X)-log(Y) [=log(X/Y)]

---

#        What is a “significant change”?
- Depends on the variability
    within groups, which may be
    different from feature to
    feature.
-   To assess the statistical
    significance of differences,
    conduct a statistical test for
    each feature.




---

#     Different settings for statistical tests

- Indirect comparisons: 2 groups, 2 samples, unpaired
    - E.g. 10 individuals: 5 suffer diabetes, 5 healthy
    - One sample fro each individual
    - Typically: Two sample t-test or similar
-   Direct comparisons: Two groups, two samples, paired
    - E.g. 6 individuals with brain stroke.
    - Two samples from each: one from healthy (region 1) and one
        from affected (region 2).
    -   Typically: One sample t-test (also called paired t-test) or
        similar, based on the individual differences between
        conditions.
---

#   “Natural” measures of discrepancy


---

#        Some issues in feature selection
- Each technology’s data may have peculiarities that
  have to be dealt with.
- Some related with small sample sizes
   -   Variance unstability
   -   Non-normality of the data
- Other related to big number of variables
   -   Multiple testing


---

#             Variance unstability
- Can we trust average effect sizes (average
      difference of means) alone?
-     Can we trust the t-statistic alone?
-     Here is evidence that the answer is no.

---

# Variance unstability (1): outliers
 - Can we trust average effect sizes (average
      difference of means) alone?
 -    Can we trust the t statistic alone?
 -    Here is evidence that the answer is no.
---

# Variance unstability (2): tiny variances
- Can we trust average effect sizes (average
       difference of means) alone?
-      Can we trust the t statistic alone?
-      Here is evidence that the answer is no.

---

#                    Solutions: Adapt t-tests
-       Let
    -     Rg mean observed log ratio
    -     SEg standard error of Rg estimated from data on gene g.
    -     SE standard error of Rg estimated from data across all
          genes.
-       Global t-test:           t=Rg/SE
-       Gene-specific t-test     t=Rg/SEg


---

# Some pro’s and con’s of t-test
      Test            Pro’s             Con’s

  Global t-test: Yields stable    Assumes variance
                 variance         homogeneity 
    t=Rg/SE      estimate          biased if false

 Gene-specific: Robust to          Low power
   t=Rg/SEg     variance           Yields unstable
                  heterogeneity   variance estimates
                                  (due to few data)

---

#         T-tests extensions


---

#     Up to here…: Can we generate a list of
             candidate features?
With the tools we have, the reasonable steps to generate a
list of candidate features may be:


We need an idea of how significant are these values
We’d like to assign them p-values

                                                                                 ---
                                                                                 
                                                                                 #                     Nominal p-values
                                                                                 
- After a test statistic is computed, it is convenient to convert it to
   a p-value:

   The probability that a test statistic, say S(X), takes values equal
   or greater than the observed value, say X0, under the
   assumption that the null hypothesis is true

                p=P{S(X)>=S(X0)|H0 true}


---

#               Significance testing

- Test of significance at the a level:
  - Reject the null hypothesis if your p-value is smaller
    than the significance level
  - It has advantages but not free from criticisms
- Genes with p-values falling below a prescribed
  level may be regarded as significant



#                 Hypothesis testing overview
            Calculation of p-values

- Standard methods for calculating p-values:
   (i) Refer to a statistical distribution table
       (Normal, t, F, …) or
   (ii) Perform a permutation analysis

---

#               (i) Tabulated p-values
- Tabulated p-values can be obtained for standard test
  statistics (e.g.the t-test)
- They often rely on the assumption of normally distributed
  errors in the data
- This assumption can be checked (approximately) using a
   -   Histogram
   -   Q-Q plot

---
                                                         20
#              Example (1)




Two groups 27 A vs 11 B samples, 3051 features
A t-test yields 1045 features with p< 0.05


---

#        (ii) Permutations tests

- Based on data shuffling. No assumptions
  - Random interchange of labels between samples
  - Estimate p-values for each comparison (feature) by
      using the permutation distribution of the t-statistics
- Repeat for every possible permutation, b=1…B
  -   Permute the n data points for feature (x). The first n1 are referred
      to as “treatments”, the second n2 as “controls”
  -   For each gene, calculate the corresponding two sample
      t-statistic, tb
- After all the B permutations are done put
        p = #{b: |tb| ≥ |tobserved|}/B

---

# Permutation tests (2)




---

#                The volcano plot:
           fold change vs log(odds)1




Significant change detected                                No change detected
               1: log(odds) is proportional to “-log (p-value)”                 
---

class: inverse, middle, center

name: multipletesting

# Multipletesting
  
---
#    How far can we trust the decision?

- The test: "Reject H0 if p-val ≤ a"
   -   is said to control the type I error because, under a
       certain set of assumptions,
       the probability of falsely rejecting H0 is less than a
       fixed small threshold
           P[Reject H0|H0 true]=P[FP] ≤ a
   -   Nothing is warranted about P[FN]
        -   “Optimal” tests are built trying to minimize this
            probability
        -   In practical situations it is often high

---

# What if we wish to test more than one gene at once? (1)

   - Consider more than one test at once
      -   Two tests each at 5% level. Now probability of getting
          a false positive is 1 – 0.95*0.95 = 0.0975
      -   Three tests       1 – 0.953 =0.1426
      -   n tests  1 – 0.95n
      -   Converge towards 1 as n increases
   - Small p-values don’t necessarily imply
     significance!!!  We are not controlling the
     probability of type I error anymore

---

# What if we wish to test more than one gene at once?
                  (2): a simulation

 - Simulation of this process for 6,000 features with 8
    treatments and 8 controls
 - All the gene expression values were simulated i.i.d from a
    N (0,1) distribution, i.e. NOTHING is significantly different
    expressed in our simulation
 - The number of features falsely identified as distinct will be
    on the average of (6000 · a), i.e. if we wanted to reject all
    genes with a p-value of less than 1% we would falsely
    reject around 60 genes
---

#    Type I error not useful in multiple testing

---

# Multiple testing: Counting errors

---

# How does type I error control extend to multiple testing
                      situations?

- Selecting genes with a p-value less than a doesn’t
  control for P[FP] anymore
- What can be done?
   -   Extend the idea of type I error
        -   FWER and FDR are two such extensions
   -   Look for procedures that control the probability for these
       extended error types
        -   Mainly adjust raw p-values

---

#        Two main error rate extensions

- Family Wise Error Rate (FWER)
   -   FWER is probability of at least one false positive
        FWER= Pr(# of false discoveries >0) = Pr(V>0)
- False Discovery Rate (FDR)
   -   FDR is expected value of proportion of false positives
       among rejected null hypotheses
        FDR = E[V/R; R>0] = E[V/R | R>0]·P[R>0]

---

#       FDR and FWER controlling procedures

- FWER
  -   Bonferroni (adj Pvalue = min{n*Pvalue,1})
  -   Holm (1979)
  -   Hochberg (1986)
  -   Westfall & Young (1993) maxT and minP
- FDR
  -   Benjamini & Hochberg (1995)
  -   Benjamini & Yekutieli (2001)

---

#                  Difference between controlling 
                         FWER or FDR
- FWER Controls for no (0) false positives
     - gives many fewer genes (false positives),
     - but you are likely to miss many
     - adequate if goal is to identify few features that differ between two groups
-   FDR Controls the proportion of false positives
     - if you can tolerate more false positives
     - you will get many fewer false negatives
     - adequate if goal is to pursue the study e.g. to determine functional
        relationships among features.

---

#          More on the False Discovery Rate

- FDR is the expected proportion of “False Positives” that is
    of the apparently significant findings really due to chance.
-   Compare to Bonferroni correction which is a bound on the
    probability that any one of the observed enrichments could
    be due to random chance.
-   Typically FDR corrections are calculated using the
    Benjamini-Hochberg procedure.
-   FDR threshold is often called the “q-value
---

#     Reducing adjustment stringency
- The adjustment to the P-value threshold
    depends on the # of tests that you do,
-   So, no matter what, the more tests you do, the
    more sensitive the test needs to be
-   Can control the stringency by reducing the
    number of tests:
     - Don’t use all features available (e.g. by filtering)
     
---

# Steps to generate a list of candidate features revisited (2)

---

#                Example (1b)




Two groups 27 A vs 11 B samples, 3051 features
Bonferroni adjustment: 98 genes with padj< 0.05 (praw < 0.000016)

---

# Questions?
# 